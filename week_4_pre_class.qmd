# Week 4: Import & Tidy Your Data (Pre-Class) {#sec-week4_pre_class}

This pre-class preparation should take about 45-60 minutes to complete.

## Overview

Getting data into R and preparing it for analysis is often your first challenge in any project. In this pre-class session, we'll focus on importing data from spreadsheets and establishing good practices for data cleaning. You'll learn reliable workflows that make your analysis more reproducible and easier to maintain.

### Video Lecture
Watch this video lecture before our interactive session:

::: {.column-page}
{{< video https://youtu.be/PLACEHOLDER >}}
:::

## Learning Objectives

By completing this pre-class work, you will:

1. Learn to import data from Excel and CSV files into R
2. Set up organized project structures
3. Establish reliable data cleaning workflows
4. Standardize common variables like dates and country names
5. Practice with real Chinese development finance data

## The Power of Clean Data

### Why Data Cleaning Matters

Data cleaning might not be anyone's idea of fun, but here's a secret: getting good at data cleaning is one of the highest-return skills you can develop as a researcher. Why? Because while lots of people can run sophisticated analyses on clean datasets, far fewer people can reliably turn messy real-world data into analysis-ready information.

At AidData, this matters even more. AidData's mission isn't just to analyze existing datasets - you create new ones that reveal previously hidden patterns in Chinese development finance. The ability to clean and standardize messy data is core to this mission.

### The Tidyverse Advantage

The tidyverse provides an extraordinarily powerful toolkit for importing and cleaning data, with specialized packages designed specifically for common cleaning tasks:

#### Data Import Tools
- [{readr}](https://readr.tidyverse.org/) ([cheatsheet](https://rawgit.com/rstudio/cheatsheets/main/data-import.pdf)): CSV and flat files
- [{readxl}](https://readxl.tidyverse.org/): Excel files
- [{haven}](https://haven.tidyverse.org/): SPSS, Stata, and SAS files
- [{httr2}](https://httr2.r-lib.org/): Web APIs

These are just the basics. There's a whole universe of tidyverse-style data import packages for all varieties of file formats and APIs (*"There's a package for that..."*). 

#### Data Cleaning Specialists
- [{stringr}](https://stringr.tidyverse.org/) ([cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf)): Text cleaning and manipulation
- [{lubridate}](https://lubridate.tidyverse.org/) ([cheatsheet](https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf)): Dates and times
- [{forcats}](https://forcats.tidyverse.org/) ([cheatsheet](https://rstudio.github.io/cheatsheets/factors.pdf)): Factor handling
- [{tidyr}](https://tidyr.tidyverse.org/) ([cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/tidyr.pdf)): Data structure tools

All of these packages follow consistent principles and are designed for humans to use. The [R for Data Science (2e)](https://r4ds.hadley.nz/) book has excellent chapters on each:

- [Data Import](https://r4ds.hadley.nz/data-import)
- [Strings](https://r4ds.hadley.nz/strings)
- [Factors](https://r4ds.hadley.nz/factors)
- [Dates and Times](https://r4ds.hadley.nz/datetimes)

### The Compounding Value of Clean Data

Here's why mastering data cleaning is worth your time:

1. **Unique Insights**: When everyone works with the same clean datasets, it's hard to find unique patterns. The ability to clean messy data gives you access to information others might miss.

2. **Reproducible Work**: Good data cleaning isn't just about getting the data right once - it's about creating reproducible pipelines that can handle new data as it arrives.

3. **Time Investment**: While cleaning data takes time upfront, having clean, reliable data saves countless hours of troubleshooting and redoing analyses later.

4. **Competitive Advantage**: In research, the quality of your inputs often matters more than the sophistication of your analysis. Being good at data cleaning lets you work with sources others might avoid.

### What We'll Learn

In the following sections, you'll learn to:

1. Import data from various sources
2. Create reliable cleaning pipelines
3. Handle common data issues like:
   - Inconsistent text formats
   - Messy dates and times
   - Different monetary formats
   - Variant country names
   
While data cleaning may never be the most exciting part of research, by the end of this section, you'll have the tools to make it a manageable, reliable part of your workflow.

## Setup

Let's get our workspace ready:

1. Create a new Quarto document for your notes:
```r
# File → New File → Quarto Document
# Save as "week_4_import_preclass.qmd" in your week_4/R folder
```

2. Create folders for organizing data:
```r
dir.create("data-raw", showWarnings = FALSE)  # For original data
dir.create("data", showWarnings = FALSE)      # For cleaned data
```

3. Download the [AidData Critical Minerals Dataset](https://www.aiddata.org/data/aiddata-chinese-financing-for-transition-minerals-dataset-version-1-0) and save it to your `data-raw` folder.

::: {.callout-tip}
## Why Two Data Folders?

- `data-raw/`: Store original data exactly as received
  - Never modify these files
  - Serves as your "source of truth"
  - Makes your work reproducible
  
- `data/`: Store cleaned, analysis-ready data
  - Modified through documented R code
  - Ready for analysis
  - Can always recreate from raw data
:::

4. Load required packages:
```{r}
#| message: false
library(tidyverse)    # Core data science tools
library(readxl)       # For reading Excel files
library(readr)        # For reading CSV files
library(janitor)      # For cleaning column names
library(countrycode)  # For standardizing country names
```

## Core Concepts

### Organizing Your Data Pipeline
Think of data cleaning as a multi-step recipe. Just like cooking, you want to:

1. Get your ingredients (import raw data, save in `data-raw`)
2. Prep them properly (clean data)
3. Create something useful (analysis-ready data, save in `data` for further use)

Let's see how this works with real data.

### Reading Data from Spreadsheets

The two main functions for reading spreadsheet data are:

- `readxl::read_excel()` for Excel files
- `readr::read_csv()` for CSV files

Let's try reading our Critical Minerals data:

```{r}
# Read the Excel file
minerals_raw <- read_excel(
  "data-raw/AidData_Chinese_Financing_for_Transition_Minerals_Dataset_Version_1.0.xlsx",
  sheet = "Financial_Contribution",
  na = c("", "NA", "N/A", "#N/A", "NULL"),  # Handle missing values
  guess_max = 20000  # Look at more rows when guessing types
)

# Quick look at what we imported
glimpse(minerals_raw)
```

::: {.callout-tip}
## Key Import Arguments

1. `na =`: What values should be treated as missing
2. `guess_max =`: How many rows to check when determining column types
3. `sheet =`: Which Excel sheet to read
4. `range =`: Specific cells to read

These help handle common import challenges like:
- Different representations of missing data
- Incorrect column type detection
- Multiple sheets in one file
:::

::: {.callout-important}
## Read the Documentation!

While we're covering key arguments, the `readxl` and `readr` packages have many more options. Taking time to read the documentation ([`read_excel()`](https://readxl.tidyverse.org/reference/read_excel.html), [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.html)) will save you hours of troubleshooting later.

You don't need to memorize all options - just know they exist and where to find them when needed.
:::

## A Systematic Approach to Data Cleaning

### Step 1: Clean Column Names & Inspect Data
First, we always want consistent, programming-friendly column names. `glimpse()` your new clean names, and look for data that might need to be cleaned.  Note any variables where the data type displayed does not match what it logically should be (e.g. is a numeric variable show with data type `<chr>`?)

```{r}

# Clean the names
minerals_clean <- minerals_raw |>
  clean_names()

# See the difference
minerals_clean |> glimpse()
```

::: {.callout-note}
## Why Clean Column Names?

Raw data often comes with inconsistent column names:

- Spaces ("Project Name")
- Special characters ("Amount ($)")
- Inconsistent capitalization ("projectName", "Project_name")
- Numbers at start ("2021_amount")

`clean_names()`:

- Converts to lowercase
- Replaces spaces/special chars with underscores
- Makes names programming-friendly
- Creates consistent style

This matters because:

- Reduces coding errors
- Makes autocomplete work better
- Prevents quoting/escaping headaches
- Creates consistent style across projects
:::

### Step 2: Fix Dates
Always inspect and clean date columns early:

```{r}
# First, find all date columns
minerals_clean |> 
  select(contains("date")) |> 
  glimpse()

# Convert all dates to proper format
minerals_dates <- minerals_clean |>
  mutate(
    commitment_date_mm_dd_yyyy = ymd(commitment_date_mm_dd_yyyy),
    planned_implementation_start_date_mm_dd_yyyy = ymd(planned_implementation_start_date_mm_dd_yyyy),
    actual_implementation_start_date_mm_dd_yyyy = ymd(actual_implementation_start_date_mm_dd_yyyy),
    planned_completion_date_mm_dd_yyyy = ymd(planned_completion_date_mm_dd_yyyy),
    actual_completion_date_mm_dd_yyyy = ymd(actual_completion_date_mm_dd_yyyy),
    first_loan_repayment_date = ymd(first_loan_repayment_date),
    last_loan_repayment_date = ymd(last_loan_repayment_date)
  )

minerals_dates |> 
  select(contains("date")) |> 
  glimpse()
```

::: {.callout-note}
## Why Clean Dates?

Raw dates can appear in many formats:

- "2021-01-15"
- "1/15/21"
- "15 Jan 2021"
- "2021-Q1"

Proper date formatting enables:

- Time-series analysis
- Duration calculations
- Correct sorting
- Filtering by time periods

If you are creating a data cleaning pipeline that will be used on newer versions of the same dataset, make sure to coerce dates into the correct format even if they are parsed correctly by `read_excel()` (or other data import methods). Next time they might not be, and a date variable that is read in as a character string might mess up your subsequent analysis pipelines. 
:::

### Step 3: Standardize Country Information
Create consistent country identifiers:

::: {.callout-note}
## Why Standardize Country Names?

Countries often appear differently across datasets:

- "Cote d'Ivoire" vs "Ivory Coast"
- "Democratic Republic of the Congo" vs "DR Congo"
- "People's Republic of China" vs "China"

Standardization enables:

- Joining across datasets
- Consistent visualization labels
- Regional aggregation
- Integration with other global data
:::

```{r}
minerals_countries <- minerals_dates |>
  mutate(
    # Add ISO3C codes
    iso3c = countrycode(
      sourcevar = recipient,
      origin = "country.name",
      destination = "iso3c",
      origin_regex = TRUE,  # Helps match variations
      warn = TRUE          # Shows what doesn't match
    ),
    # Add standardized names
    country_name = countrycode(
      sourcevar = iso3c,
      origin = "iso3c",
      destination = "country.name"
    ),
    # Add World Bank regions
    wb_region = countrycode(
      sourcevar = iso3c,
      origin = "iso3c",
      destination = "region"
    )
  )

minerals_countries |> 
  select(
    country_name,
    recipient,
    wb_region,
    recipient_region
  ) |> 
  unique()
```
::: {.callout-warning}
## Handling Non-Country Entries

Some datasets include regional entries (like "Africa, regional") that won't match country codes. Options for handling these:

1. Use `warn = TRUE` to see what doesn't match
2. Create custom matching rules for special cases (see `?countrycode`)
3. Document any manual corrections needed

How you handle these will depend on the context of your dataset and analysis. It's worth remembering that the definition of regions and other aggregates often varies slightly by data source. 
:::


::: {.callout-tip}
## Country Code Best Practices

1. **Use ISO3C for Programming**
   - Three-letter codes are unambiguous
   - Avoid ISO2C (e.g., Namibia's "NA" can cause issues)
   - Perfect for plot labels where space is tight

2. **Use Standardized Names for Presentation**
   - More readable than codes
   - Consistent across datasets
   - Good for reports and visualizations

3. **Keep Multiple Identifiers**
   - Original names (match documentation)
   - ISO3C codes (for programming)
   - Standardized names (for presentation)
   - Regional groupings (for analysis)

4. **Check the `countrycode::codelist`**
   ```r
   # See all available code types
   ?countrycode::codelist
   
   # Common useful conversions:
   # - "continent" Continent as defined in the World Bank Development Indicators
   # - "currency" ISO 4217 currency name
   # - "region" Regions as defined in the World Bank Development Indicators
   # - "eu28" for EU membership
   ```
:::



### Step 4: Create Proper Categories
Convert text categories to meaningful factors. 

You don't need to do this for all text variables, but consider doing it for ones you are going to use often, and where order matters for tables + charts. 

```{r}
minerals_cats <- minerals_countries |>
  mutate(
    # Make status an ordered factor
    status = factor(
      status,
      levels = c(
        "Pipeline: Commitment",
        "Implementation",
        "Completion"
      )
    ),
    # Make income groups ordered
    oecd_oda_income_group = factor(
      oecd_oda_income_group,
      levels = c(
        "Low income",
        "Lower middle income",
        "Upper middle income"
      )
    )
  )

minerals_cats |> 
  count(
    status
  )
```

::: {.callout-note}
## Why Create Proper Factors?

Raw categorical data often needs structure:

- Natural ordering (status phases)
- Grouping levels (income categories)
- Consistent labels

Proper factors enable:

- Correct ordering in plots
- Meaningful summaries
- Efficient filtering
- Clear presentation
:::

## Creating a Reusable Pipeline

Now let's combine these steps into a reusable function:

```{r}
process_minerals_data <- function(data) {
  data |>
    # Step 1: Clean column names
    clean_names() |>
    
    # Step 2: Fix dates
    mutate(
      commitment_date_mm_dd_yyyy = ymd(commitment_date_mm_dd_yyyy),
      planned_implementation_start_date_mm_dd_yyyy = ymd(planned_implementation_start_date_mm_dd_yyyy),
      actual_implementation_start_date_mm_dd_yyyy = ymd(actual_implementation_start_date_mm_dd_yyyy),
      planned_completion_date_mm_dd_yyyy = ymd(planned_completion_date_mm_dd_yyyy),
      actual_completion_date_mm_dd_yyyy = ymd(actual_completion_date_mm_dd_yyyy),
      first_loan_repayment_date = ymd(first_loan_repayment_date),
      last_loan_repayment_date = ymd(last_loan_repayment_date)
    ) |>
    
    # Step 3: Standardize country information
    mutate(
      iso3c = countrycode(
        sourcevar = recipient,
        origin = "country.name",
        destination = "iso3c",
        origin_regex = TRUE,
        warn = TRUE
      ),
      country_name = countrycode(
        sourcevar = iso3c,
        origin = "iso3c",
        destination = "country.name"
      ),
      wb_region = countrycode(
        sourcevar = iso3c,
        origin = "iso3c",
        destination = "region"
      )
    ) |>
    
    # Step 4: Create proper factors
    mutate(
      status = factor(
        status,
        levels = c(
          "Pipeline: Commitment",
          "Implementation",
          "Completion"
        ),
      ),
      # Make income groups ordered
      oecd_oda_income_group = factor(
        oecd_oda_income_group,
        levels = c(
          "Low income",
          "Lower middle income",
          "Upper middle income"
        )
      ) 
    ) |>
    
    # Step 5: Add derived variables
    mutate(
      amount_bn = amount_constant_usd_2021 / 1e9
    )
}

# Use the pipeline
# First import the data
minerals_raw <- read_excel(
  here::here(
    "data-raw", 
    "AidData_Chinese_Financing_for_Transition_Minerals_Dataset_Version_1.0.xlsx"
  ),
  sheet = "Financial_Contribution",
  na = c("", "NA", "N/A", "#N/A", "NULL"),
  guess_max = 20000
)

# Then process it
minerals_clean <- minerals_raw |> 
  process_minerals_data()

# Save the results
write_rds(
  minerals_clean, 
  here::here("data", "minerals_clean.rds")
)

write_csv(
  minerals_clean, 
  here::here("data", "minerals_clean.csv")
)
```

## Data Cleaning Checklist

Before considering your data clean, verify:

**Data Structure**

- [ ] Column names are clean and consistent
- [ ] Each variable is in its own column
- [ ] Each observation is in its own row
- [ ] No duplicate observations
- [ ] No hidden formatting/calculations (if from Excel)

**Data Types**

- [ ] Dates are proper date objects
- [ ] Numbers are numeric (not text)
- [ ] Categories are proper factors
- [ ] Text fields are character type
- [ ] No mixed types in columns

**Standardization**

- [ ] Country names/codes are consistent
- [ ] Categories have standardized values
- [ ] Units are consistent
- [ ] Missing values are properly coded
- [ ] Special characters handled appropriately

**Documentation**

- [ ] Cleaning steps are documented
- [ ] Unusual values are noted
- [ ] Missing value handling is explained
- [ ] Assumptions are documented
- [ ] Output files are properly labeled

**Quality Control**

- [ ] Row counts match expectations
- [ ] Column counts match expectations
- [ ] Summaries look reasonable
- [ ] Missing value patterns make sense
- [ ] Extreme values are investigated

## Effective AI Prompts for Data Cleaning

Here are some powerful prompts that will help you get the most out of AI tools when cleaning data:

### Making Data Tidy

```markdown
I have a dataset that looks like this:
[paste first few rows of your data using head() or glimpse()]

I think it might not be in tidy format because:
[describe what seems wrong, e.g., "multiple variables in one column" or "values spread across columns"]

Can you help me:
1. Identify which tidyverse principles it violates:
   - Is each variable a column?
   - Is each observation a row?
   - Is each value a single cell?

2. Suggest a tidyr pipeline to fix it?
3. Explain why each step in the pipeline helps?
```

Example:
```markdown
I have this dataset:
Year  Q1_Sales  Q2_Sales  Q3_Sales  Q4_Sales
2021    100       120       95        150
2022    110       125       100       160

This seems untidy because sales values are spread across columns.
How can I reshape this to have columns: year, quarter, sales?
```

Tips for good tidy data prompts:

1. Show sample data
2. Explain what seems wrong
3. Describe desired output
4. Ask for explanation of steps

### Understanding Data Structure
```markdown
I have an Excel file with this glimpse() output:
[paste your glimpse() output]

I want to:
1. Clean the column names
2. Convert dates to proper format
3. Standardize country names
Can you help me write a tidyverse pipeline to do this?
```

### Date Standardization
```markdown
I have dates in these formats:
[paste unique(date_column)]

I need to:
1. Convert them to proper date objects
2. Handle missing/invalid dates
3. Create consistent format

Can you help me write a robust date cleaning function?
```

### Debugging Data Issues
```markdown
I'm trying to clean this data but getting this error:
[paste error message]

Here's my code:
[paste code]

Here's a sample of my data:
[paste glimpse(data)]

Can you help me:
1. Understand what's wrong
2. Fix the immediate issue
3. Prevent similar issues?
```

### Creating Cleaning Functions
```markdown
I need to clean multiple similar datasets with these characteristics:
[paste glimpse(data)]

Common issues include:
[list issues]

Can you help me write a robust cleaning function that:
1. Handles all these cases
2. Includes error checking
3. Documents the cleaning steps?
```

::: {.callout-tip}
## Getting the Most from AI

1. **Show Your Data**
   - Use `glimpse()`, `head()`, or `str()`
   - Include sample values
   - Show error messages

2. **Be Specific**
   - Explain your goal
   - Describe current issues
   - List any constraints

3. **Ask for Explanation**
   - Request comments in code
   - Ask about trade-offs
   - Get help with error handling
:::

## Practice Exercises

### Exercise 1: Creating a Data Processing Pipeline

Let's practice by cleaning some financial data with common issues:

```{r}
# Create our messy data
messy_data <- "
Country,Project Value,Date,Status
People's Republic of China,\"$12,000,000\",Sep 20 2021,ACTIVE
Democratic Republic of Congo,\"$8,500,000\",Sep 15 2021,Active
Vietnam,\"$15,250,000\",Sep 10 2021,COMPLETED
China,\"$9,750,000\",Sep 5 2021,active
Cote d'Ivoire,\"$11,250,000\",Sep 1 2021,Pipeline
"

# Save to data-raw using here()
write_file(
  messy_data,
  here::here("data-raw", "messy_finance.csv")
)
```

Your tasks:

1. Create a function called `process_finance_data()` that:
   - Standardizes country names (note: China appears twice with different names)
   - Cleans monetary values using `parse_number()`
   - Converts dates to proper date format using lubridate
   - Creates proper status factors. Use `str_to_title()` to get values to consistent case first.

2. Write a pipeline that:
   ```r
   # Import data
   finance_raw <- read_csv(
     here::here("data-raw", "messy_finance.csv")
   )
   
   # Process it
   finance_clean <- finance_raw |>
     process_finance_data()
   
   # Save results
   write_rds(
     finance_clean,
     here::here("data", "finance_clean.rds")
   )
   ```

::: {.callout-tip}
## Useful Cleaning Functions

**For Numbers:**
```r
# parse_number() removes currency symbols and commas
parse_number("$12,000,000")  # Returns 12000000
parse_number("$1,234.56")    # Returns 1234.56
```

**For Dates:**
Lubridate provides functions matching common date formats:

- `mdy()` (month-day-year): "Sep 20 2021" → 2021-09-20
- `ymd()` (year-month-date): "2021-09-20" → 2021-09-20
- `dmy()` (day-month-year): "20-09-2021" → 2021-09-20

```{r}
# Examples
library(lubridate)
mdy("Sep 20 2021")    # Returns "2021-09-20"
ymd("2021-09-20")     # Returns "2021-09-20"
dmy("20-09-2021")     # Returns "2021-09-20"
```

The function name matches the order of the date components (m=month, d=day, y=year).
:::

::: {.callout-tip}
## Making Text Case Consistent

The {stringr} package provides several functions for standardizing text case:

```r
# Convert to title case (First Letter Of Each Word)
str_to_title("RURAL ELECTRIFICATION project")  # Returns "Rural Electrification Project"

# Convert to upper case (ALL CAPS)
str_to_upper("Rural Electrification Project")  # Returns "RURAL ELECTRIFICATION PROJECT"

# Convert to lower case (all lowercase)
str_to_lower("Rural Electrification Project")  # Returns "rural electrification project"

# Real world example: standardizing status values
status_values <- c("IN PROGRESS", "Completed", "not started", "In Progress")

status_clean <- status_values |>
  str_to_title()  # Returns: "In Progress", "Completed", "Not Started", "In Progress"
```

When to use each:

- `str_to_title()`: Names, project titles, status values
- `str_to_upper()`: Country codes, ID values
- `str_to_lower()`: Before matching or comparing strings
:::

### Exercise 2: Working with Multiple Data Quality Issues

Let's practice handling several common data quality issues:

```{r}
# Create project data with various issues
projects <- "
Region,Project Title,Start Date,Budget (USD),Status
East Asia,Water Treatment Plant,Sep 15 2021,$12000000,In Progress
Eastern Asia,Solar Farm Phase 1,Sep 10 2021,$15250000,ACTIVE
Sub-Saharan Africa,Highway Extension,,\"$8,500,000\",planning
SSA,Rural Electrification,Sep 1 2021,$11250000,ACTIVE
"

# Save using here()
write_file(
  projects,
  here::here("data-raw", "projects.csv")
)
```

Your tasks:

1. Create a data processing function that:
   - Standardizes region names (East Asia/Eastern Asia, Sub-Saharan Africa/SSA)
   - Handles the missing date
   - Cleans monetary values
   - Standardizes status values
   - Groups similar projects by region

2. Add validation checks that:
   - Verify all required fields are present
   - Check date ranges
   - Validate budget ranges
   - Ensure status values are standardized

::: {.callout-tip}
## Common Data Quality Issues

When working with real data, watch for:

1. **Inconsistent Names**
   - Different spellings
   - Abbreviations
   - Regional variations

2. **Missing Values**
   - Empty cells
   - Placeholder values ("N/A", "-", etc.)
   - Impossible values

3. **Format Inconsistencies**
   - Mixed date formats
   - Different currency notations
   - Varied text cases

Always document how you handle each type of issue!
:::

## Resources for Learning More

### Essential References

1. **Data Import & Cleaning**
   - [R for Data Science - Data Import](https://r4ds.hadley.nz/data-import)
   - [readxl documentation](https://readxl.tidyverse.org/)
   - [janitor vignette](https://sfirke.github.io/janitor/articles/janitor.html)
   - [countrycode documentation](https://vincentarelbundock.github.io/countrycode/)

2. **Working with Dates**
   - [Lubridate vignette](https://lubridate.tidyverse.org/articles/lubridate.html)
   - [Dates and Times in R](https://r4ds.hadley.nz/dates.html)

3. **Number Formatting**
   - [readr vignette on parsing numbers](https://readr.tidyverse.org/articles/readr.html#numbers)
   - [scales package for formatting](https://scales.r-lib.org/)



## Next Steps

In our class session, we'll:

1. **Work with Complex Datasets**
   - Handle multiple related files
   - Learn about different types of joins
   - Create robust cleaning pipelines

2. **Build Validation Systems**
   - Create data quality checks
   - Validate transformations
   - Document cleaning decisions

3. **Practice with Real Data**
   - Work with your own datasets
   - Solve common challenges
   - Create reusable solutions

4. **Learn Advanced Techniques**
   - Handle special cases
   - Create custom cleaning functions
   - Build automated workflows

Remember: Good data cleaning is the foundation of reliable analysis. The time you invest in creating robust cleaning pipelines will save you hours of troubleshooting later!